# Appendix G: Biased Chatbots and Political Influence

<div style="border:1px solid #d1d5db; background-color:#f9fafb; padding:12px 16px; border-radius:8px; margin:16px 0;">
  Conversational AI tools can shift political attitudes after just a few short exchanges. While the effect on any single person may be small, the ability to reach millions cheaply and consistently makes this a powerful new force in elections and public opinion.
</div>

## What the University of Washington study found

In 2025, researchers at the University of Washington asked nearly 300 people — half Democrats, half Republicans — to chat with different versions of ChatGPT.[^uwnews][^aclpaper] One version spoke like a strong Democrat, another like a strong Republican, and a third tried to stay neutral.

- Even after just **a few back-and-forth messages**, people’s opinions shifted toward the political leaning of the chatbot they spoke with.[^aclpaper]  
- In a “mayor’s budget” exercise, participants moved money away from education and welfare and toward policing and veterans after interacting with the conservative-biased chatbot.[^aclpaper]  
- Both Democrats and Republicans were **vulnerable**, regardless of where they started politically.[^aclpaper]

<div style="border-left:6px solid #2563eb; background-color:#eff6ff; padding:12px 16px; border-radius:4px; margin:16px 0;">
  <strong>Key takeaway:</strong> Short conversations with biased AI tools measurably nudged people’s political views and spending choices.
</div>

## Why this matters

The researchers also tested whether awareness or knowledge could protect people. The results were worrying:

- People who recognized the chatbot’s political lean **were still influenced**.[^aclpaper]  
- Those with stronger background knowledge about AI were a bit less likely to be swayed — but this was only a partial shield.[^aclpaper]

<div style="border-left:6px solid #dc2626; background-color:#fef2f2; padding:12px 16px; border-radius:4px; margin:16px 0;">
  <strong>Concerning pattern:</strong> Knowing a chatbot is biased does not stop it from shaping opinions. Even awareness may not be enough defense.
</div>

## Other research pointing the same direction

The UW study is not alone. A growing set of experiments has tested how persuasive AI can be:

- **Debate settings**: GPT-4 persuaded people more often than human debaters in structured contests.[^washpost][^guardian]  
- **Partisan role-play**: In a large study, GPT-4 pretending to be “left” or “right” was about as persuasive as professional communicators.[^springer]  
- **Field trial with 8,500+ people**: GPT-4 messages shifted policy support by up to 12 percentage points. Surprisingly, **microtargeted messages were no stronger than generic ones**.[^pnasfield]  
- **Countering conspiracy beliefs**: A Science study showed chatbots reduced belief in conspiracies by about 20%, with effects lasting two months.[^science]  
- **Viewpoint diversity**: LLM-powered bots widened the range of arguments people saw in chatrooms, even when participants knew the posts came from AI.[^diversity]  
- **Bias in alignment training**: MIT researchers found that reward systems designed to “align” language models already carried left-leaning bias.[^mit]  
- **Meta-analyses**: Some reviews conclude AI is not consistently more persuasive than humans. The danger comes from **scale, targeting, and 24/7 availability**, not one-on-one superiority.[^persuasionrisk][^levers][^neutrality]

<div style="border-left:6px solid #1f2937; background-color:#f3f4f6; padding:12px 16px; border-radius:4px; margin:16px 0;">
  <strong>Balanced takeaway:</strong> AI persuasion effects are often modest, but the ability to deploy them cheaply, continuously, and at massive scale creates systemic risks for democracy.
</div>

## Big picture implications

Together, these studies suggest biased AI is not just another media channel. It combines the **reach of social media** with the **personalization of one-on-one conversation**. Traditional tools like media literacy — “spot the bias” — may not protect people. Instead, stronger defenses such as **AI literacy, transparency, and governance rules** are needed to reduce the risk of AI being misused in elections.

---

## Footnotes & Sources

[^uwnews]: University of Washington News release — “With just a few messages, biased AI chatbots swayed people’s political views” (Aug 6, 2025). https://www.washington.edu/news/2025/08/06/biased-ai-chatbots-swayed-peoples-political-views/
[^aclpaper]: ACL 2025 paper (ACL Anthology PDF) — “Biased LLMs can Influence Political Decision-Making.” Reports 299 participants, 3×2 factorial design, β coefficients for shifts, and budget allocation effects. https://aclanthology.org/2025.acl-long.328.pdf
[^washpost]: Washington Post — “AI chatbots more persuasive than humans in debates, study finds” (May 19, 2025). https://www.washingtonpost.com/technology/2025/05/19/artificial-intelligence-llm-chatbot-persuasive-debate/
[^guardian]: The Guardian — “AI can be more persuasive than humans in debates, scientists find” (May 19, 2025). https://www.theguardian.com/technology/2025/may/19/ai-can-be-more-persuasive-than-humans-in-debates-scientists-find-implications-for-elections
[^springer]: Springer AI & Society (2025) — “Partisan Role-Play with GPT-4: Persuasion at Scale.” Reports 4,955 participants, pre-registered design. https://link.springer.com/article/10.1007/s00146-025-02464-x
[^pnasfield]: PNAS (2025) — “Political Microtargeting with Generative AI: Evidence from a Field Experiment.” n = 8,587, ±12 pp shifts. https://pmc.ncbi.nlm.nih.gov/articles/PMC11181035/
[^mit]: MIT News (2024) — “Study finds language-model reward systems exhibit political bias.” https://news.mit.edu/2024/study-some-language-reward-models-exhibit-political-bias-1210
[^science]: Science (2024) — “Personalized AI chatbots reduce conspiracy beliefs in sustained field trial.” ~20% reduction, effect sustained two months. https://www.science.org/doi/10.1126/science.adq1814
[^diversity]: ArXiv (Jun 2025) — “AI Bots Expand Argument Range and Viewpoint Diversity.” https://arxiv.org/abs/2506.17073
[^persuasionrisk]: ArXiv (2025) — “A Framework to Assess the Persuasion Risks Large Language Models Pose.” https://arxiv.org/html/2505.00036v1
[^levers]: ArXiv (Jul 2025) — “The Levers of Political Persuasion with Conversational AI.” n ≈ 76,977 conversations across 19 LLMs. https://arxiv.org/html/2507.13919v1
[^neutrality]: ArXiv (2025) & OpenReview — “Political Neutrality in AI Is Impossible — But Here Is How to Approximate It.” https://arxiv.org/abs/2503.05728 ; https://openreview.net/forum?id=H72JEXAPwo
